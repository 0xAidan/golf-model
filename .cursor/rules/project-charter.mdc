---
description: Project charter — stopping rules, bootstrap, go-live gates. Refer with every deployment decision.
globs: 
alwaysApply: false
---

# Golf Model Project Charter

## Stopping Rules (SPRT-inspired)

We commit to the following stopping criteria to avoid continuing a strategy that has no edge:

1. **CLV < 0% after 150 bets:** Full investigation. Review model calibration, blend weight, and data quality before adding more capital.
2. **CLV < 0% AND negative ROI after 300 bets:** Model has no demonstrated edge; stop live betting until root cause is addressed and re-validated (e.g. paper trade).

These are documented commitments, not automated code. The calibration dashboard (`python analyze.py --calibration`) and CLV summary support monitoring.

## 4-Phase Bootstrap Protocol

| Phase         | Tournaments | Kelly Fraction | What We Track                |
|---------------|-------------|----------------|-----------------------------|
| Shadow        | 1–5         | 0 (predict only) | Brier, calibration curve   |
| Paper         | 6–15        | 0 (paper trade)  | CLV, hypothetical P/L, exposure |
| Cautious Live | 16–25       | 1/8 Kelly       | Real P/L, real CLV, drawdown |
| Full Live     | 26+         | 1/4 Kelly       | All metrics, SPRT tests    |

**Gates between phases:**
- Shadow → Paper: Brier score < 0.25, no systematic calibration bias.
- Paper → Cautious: CLV > 0% over 100+ bets, no segment regressions.
- Cautious → Full: CLV > 1% over 250+ bets, hit rate > 55%, max drawdown < 15%.

## Go-Live Hard Gates

All must pass before moving to full live (1/4 Kelly):

1. Minimum 250 tracked bets.
2. Average CLV > 1%.
3. CLV hit rate > 55%.
4. Brier score < 0.22 for matchups.
5. No market segment with Brier > 0.28.
6. Maximum drawdown during paper trading < 20%.

Soft gates (desirable): positive paper P/L; at least 3 market types tracked.

## Shadow Mode

For model evolution, run the pipeline in shadow mode (e.g. alternate config or blend) in parallel with production. Compare output cards and Brier/CLV before promoting changes. Implementation: run pipeline twice with different config (e.g. env or feature flags) and diff outputs.

## Walk-Forward Backtesting

The only valid backtest approach is walk-forward: train on data available before each event, predict, then advance. Do not use future data. The backtester should use the same model code as live (`src/models/` with PIT data). Building PIT data for 200+ historical events is a multi-day data task; use `backtester/pit_models.py` (which imports from `src/models/` and config) for consistency.
